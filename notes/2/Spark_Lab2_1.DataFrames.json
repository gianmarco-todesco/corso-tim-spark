{"paragraphs":[{"text":"%md\n# Prendiamo confidenza con Spark! #","user":"bigdata","dateUpdated":"2019-04-08T21:26:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Prendiamo confidenza con Spark!</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1554756006288_893512700","id":"20190408-204006_707278487","dateCreated":"2019-04-08T20:40:06+0000","dateStarted":"2019-04-08T21:26:38+0000","dateFinished":"2019-04-08T21:26:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:728"},{"text":"// la variabile spark contiene la SparkSession\nspark","user":"bigdata","dateUpdated":"2019-04-08T21:26:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6daa32f1\n"}]},"apps":[],"jobName":"paragraph_1554756037356_-765418661","id":"20190408-204037_1768731740","dateCreated":"2019-04-08T20:40:37+0000","dateStarted":"2019-04-08T21:26:41+0000","dateFinished":"2019-04-08T21:27:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:729"},{"text":"// la quale a sua volta contiene lo SparkContext\nspark.sparkContext","user":"bigdata","dateUpdated":"2019-04-08T21:27:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res2: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3132b790\n"}]},"apps":[],"jobName":"paragraph_1554756051897_-777839123","id":"20190408-204051_286345339","dateCreated":"2019-04-08T20:40:51+0000","dateStarted":"2019-04-08T21:27:05+0000","dateFinished":"2019-04-08T21:27:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:730"},{"text":"// il quale è disponibile anche direttamente con la variabile sc\nsc","user":"bigdata","dateUpdated":"2019-04-08T21:27:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res3: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3132b790\n"}]},"apps":[],"jobName":"paragraph_1554756075119_-2080068403","id":"20190408-204115_1572965775","dateCreated":"2019-04-08T20:41:15+0000","dateStarted":"2019-04-08T21:27:05+0000","dateFinished":"2019-04-08T21:27:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:731"},{"text":"// Possiamo creare un Dataset che contiene il primo milione di interi (0 compreso)\nval ds = spark.range(1000000)","user":"bigdata","dateUpdated":"2019-04-08T21:27:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ds: org.apache.spark.sql.Dataset[Long] = [id: bigint]\n"}]},"apps":[],"jobName":"paragraph_1554756099666_-870158284","id":"20190408-204139_569367414","dateCreated":"2019-04-08T20:41:39+0000","dateStarted":"2019-04-08T21:27:06+0000","dateFinished":"2019-04-08T21:27:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:732"},{"text":"// il tipo è bigint, ma significa Long (intero a doppia precisione). Ci sono problemi di overflow\nds.map(x=>x*x*x).reduce(_+_)","user":"bigdata","dateUpdated":"2019-04-08T21:27:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res4: Long = -9222430735553051648\n"}]},"apps":[],"jobName":"paragraph_1554756138159_-1323082806","id":"20190408-204218_1956065758","dateCreated":"2019-04-08T20:42:18+0000","dateStarted":"2019-04-08T21:27:09+0000","dateFinished":"2019-04-08T21:27:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:733","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.30.0.97:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}}},{"text":"// che posso risolvere nel solito modo (usando il tipo BigDecimal, che Spark conosce)\nds.map(x=>BigDecimal(x)).map(x=>x*x*x).reduce(_+_)","user":"bigdata","dateUpdated":"2019-04-08T21:27:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res5: scala.math.BigDecimal = 249999500000250000000000\n"}]},"apps":[],"jobName":"paragraph_1554756189203_702082508","id":"20190408-204309_509716238","dateCreated":"2019-04-08T20:43:09+0000","dateStarted":"2019-04-08T21:27:18+0000","dateFinished":"2019-04-08T21:27:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:734","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.30.0.97:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}}},{"text":"// il Dataset è una struttura tipizzata (in questo caso sono Interi a doppia precisione). Posso convertirlo in un Dataframe\nval df = ds.toDF(\"x\")","user":"bigdata","dateUpdated":"2019-04-08T21:27:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [x: bigint]\n"}]},"apps":[],"jobName":"paragraph_1554756281473_-1809757998","id":"20190408-204441_866821442","dateCreated":"2019-04-08T20:44:41+0000","dateStarted":"2019-04-08T21:27:20+0000","dateFinished":"2019-04-08T21:27:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:735"},{"text":"// il DataFrame \"sa\" di che tipo sono le sue colonne, ma solo a run-time.\n// Quest'informazione si chiama \"schema\"\ndf.printSchema()","user":"bigdata","dateUpdated":"2019-04-08T21:27:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- x: long (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1554756388456_-111714028","id":"20190408-204628_1144788635","dateCreated":"2019-04-08T20:46:28+0000","dateStarted":"2019-04-08T21:27:20+0000","dateFinished":"2019-04-08T21:27:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:736"},{"text":"// è possibile creare tabelle a più colonne\nval df2 = Seq(\n    (\"rosso\",1,10.0),\n    (\"verde\",2,15.0),\n    (\"blu\",3,6.23))\n    .toDF(\"colore\", \"codice\", \"valore\")\ndf2.printSchema()\ndf2.show()","user":"bigdata","dateUpdated":"2019-04-08T21:27:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- colore: string (nullable = true)\n |-- codice: integer (nullable = false)\n |-- valore: double (nullable = false)\n\n+------+------+------+\n|colore|codice|valore|\n+------+------+------+\n| rosso|     1|  10.0|\n| verde|     2|  15.0|\n|   blu|     3|  6.23|\n+------+------+------+\n\ndf2: org.apache.spark.sql.DataFrame = [colore: string, codice: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1554756439881_-433564211","id":"20190408-204719_372517007","dateCreated":"2019-04-08T20:47:19+0000","dateStarted":"2019-04-08T21:27:21+0000","dateFinished":"2019-04-08T21:27:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:737"},{"text":"// come dicevamo, il tipo non è noto a tempo di compilazione\n// quindi ad es non è possibile fare direttamente delle operazioni aritmetiche sui valori (numerici) della terza colonna.\ndf2.first()(2)\n\n","user":"bigdata","dateUpdated":"2019-04-08T21:27:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res8: Any = 10.0\n"}]},"apps":[],"jobName":"paragraph_1554756527395_-1892436292","id":"20190408-204847_1194191844","dateCreated":"2019-04-08T20:48:47+0000","dateStarted":"2019-04-08T21:27:22+0000","dateFinished":"2019-04-08T21:27:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:738"},{"text":"// è necessario fare un cast.\ndf2.first.getDouble(2) * 2","user":"bigdata","dateUpdated":"2019-04-08T21:27:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res9: Double = 20.0\n"}]},"apps":[],"jobName":"paragraph_1554756713477_-1826584160","id":"20190408-205153_1652203031","dateCreated":"2019-04-08T20:51:53+0000","dateStarted":"2019-04-08T21:27:22+0000","dateFinished":"2019-04-08T21:27:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:739"},{"text":"df2.schema","user":"bigdata","dateUpdated":"2019-04-08T21:27:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res10: org.apache.spark.sql.types.StructType = StructType(StructField(colore,StringType,true), StructField(codice,IntegerType,false), StructField(valore,DoubleType,false))\n"}]},"apps":[],"jobName":"paragraph_1554757829599_-286809590","id":"20190408-211029_991585028","dateCreated":"2019-04-08T21:10:29+0000","dateStarted":"2019-04-08T21:27:23+0000","dateFinished":"2019-04-08T21:27:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:740"},{"text":"// per passare ad un Dataset ci serve una case class. La usiamo come \"schema\", però noto a tempo di compilazione\nimport spark.implicits._\ncase class MioRecord(colore:String, codice:Integer, valore:Double)\n","user":"bigdata","dateUpdated":"2019-04-08T21:27:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\ndefined class MioRecord\n"}]},"apps":[],"jobName":"paragraph_1554758278671_2126518846","id":"20190408-211758_1105328250","dateCreated":"2019-04-08T21:17:58+0000","dateStarted":"2019-04-08T21:27:23+0000","dateFinished":"2019-04-08T21:27:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:741"},{"text":"\nval ds2 = df2.as[MioRecord]\nds2.show()\n// posso accedere direttamente al valore numerico della terza colonna del primo record\nds2.first.valore*2","user":"bigdata","dateUpdated":"2019-04-08T21:28:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------+------+\n|colore|codice|valore|\n+------+------+------+\n| rosso|     1|  10.0|\n| verde|     2|  15.0|\n|   blu|     3|  6.23|\n+------+------+------+\n\nds2: org.apache.spark.sql.Dataset[MioRecord] = [colore: string, codice: int ... 1 more field]\nres12: Double = 20.0\n"}]},"apps":[],"jobName":"paragraph_1554756752203_1434367759","id":"20190408-205232_994140406","dateCreated":"2019-04-08T20:52:32+0000","dateStarted":"2019-04-08T21:27:43+0000","dateFinished":"2019-04-08T21:27:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:742"},{"user":"bigdata","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554756926164_973491214","id":"20190408-205526_2061505024","dateCreated":"2019-04-08T20:55:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:743","text":"// dataframe e dataset hanno un rdd soggiacente\nprintln(ds2.rdd)\nprintln(df2.rdd)","dateUpdated":"2019-04-08T21:29:03+0000","dateFinished":"2019-04-08T21:29:04+0000","dateStarted":"2019-04-08T21:29:04+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"MapPartitionsRDD[14] at rdd at <console>:33\nMapPartitionsRDD[18] at rdd at <console>:36\n"}]}},{"user":"bigdata","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554758943966_760914259","id":"20190408-212903_97202839","dateCreated":"2019-04-08T21:29:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2554","text":"// posso creare direttamente un rdd, specificando anche il numero di partizioni\nval rdd = sc.parallelize(Seq(1 to 1000), 4)\nprintln(rdd)\nprintln(rdd.getNumPartitions)","dateUpdated":"2019-04-08T21:30:20+0000","dateFinished":"2019-04-08T21:30:21+0000","dateStarted":"2019-04-08T21:30:20+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ParallelCollectionRDD[19] at parallelize at <console>:29\n4\nrdd: org.apache.spark.rdd.RDD[scala.collection.immutable.Range.Inclusive] = ParallelCollectionRDD[19] at parallelize at <console>:29\n"}]}},{"user":"bigdata","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554759020534_-663170685","id":"20190408-213020_1298642283","dateCreated":"2019-04-08T21:30:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2651","text":"// Posso creare un dataframe a partire da dati e schema\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\nval dati = spark.sparkContext.parallelize(Seq(\n    Row(\"rosso\", 1),\n    Row(\"verde\", 2),\n    Row(\"blu\", 3)))\nval schema = StructType(Seq(\n    StructField(\"colore\", StringType, false),\n    StructField(\"codice\", IntegerType, false)))\nval df3 = spark.createDataFrame(dati, schema)\ndf3.show()\n","dateUpdated":"2019-04-08T21:35:08+0000","dateFinished":"2019-04-08T21:35:16+0000","dateStarted":"2019-04-08T21:35:08+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------+\n|colore|codice|\n+------+------+\n| rosso|     1|\n| verde|     2|\n|   blu|     3|\n+------+------+\n\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\ndati: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[24] at parallelize at <console>:29\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(colore,StringType,false), StructField(codice,IntegerType,false))\ndf3: org.apache.spark.sql.DataFrame = [colore: string, codice: int]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.30.0.97:4040/jobs/job?id=3","http://172.30.0.97:4040/jobs/job?id=4"],"interpreterSettingId":"spark"}}},{"user":"bigdata","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1554759235448_-1989759872","id":"20190408-213355_898550800","dateCreated":"2019-04-08T21:33:55+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2877"}],"name":"Spark/Lab2/1.DataFrames","id":"2EA5RNRJT","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}